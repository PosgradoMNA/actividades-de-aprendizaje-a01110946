{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "colab_type": "text",
        "id": "view-in-github"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/PosgradoMNA/actividades-de-aprendizaje-a01110946/blob/main/assignment01_Python-for-Data-Science-IBM.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "# **Ciencia y analítica de Datos**\n",
        "#### A01110946 | Maytorena Espinosa de los Monteros, Fernando\n",
        "#### Curso de IBM: \"Data Analysis with Python\"\n",
        "#### Profesor: María De la Paz Rico\n",
        "#### Tutor: Roberto Antonio Guevara González"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "md9dN8w6WzRV"
      },
      "source": [
        "## **Actividad de semana 4: Data Analysis with Python (IBM)**\n",
        "---\n",
        "## Apuntes\n",
        "#### Estos son los apuntes que tomé del primer módulo del curso de Data Anlysis with Python (IBM).\n",
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qmTwL3YuYvU6"
      },
      "source": [
        "## Module 1: Introduction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### Introduction to Data Analysis with Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Problem: Used Car Appraisal\n",
        "##### Can we estimate the price of used cars?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# primero importamos las librerías necesarias\n",
        "import pandas as pd\n",
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# después leemos el archivo csv\n",
        "path = \"https://cf-courses-data.s3.us.cloud-object-storage.appdomain.cloud/IBMDeveloperSkillsNetwork-DA0101EN-SkillsNetwork/labs/Data%20files/auto.csv\""
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# cargamos el archivo al notebook para iniciar a trabajar con sus datos, y se lo asignamos a la variable \"df\"\n",
        "# es importante agregar el argumento header=None para que no tome la primera fila como encabezado\n",
        "\n",
        "df = pd.read_csv(path, header=None)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# para iniciar con el análisis de los datos, solicitamos ver las primeras cinco filas utilizando el métodod \"dataframe.head()\"\n",
        "print(\"Las primeras cinco filas del dataframe son:\")\n",
        "df.head(5)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# también podemos analizar las últimas diez filas utilizando el método \"dataframe.tail(10)\"\n",
        "print(\"Las últimas diez filas del dataframe son:\")\n",
        "df.tail(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#como podemos observar, el dataframe no tiene encabezado, por lo que debemos agregarlo manualmente.\n",
        "# para esto, utilizamos el método \"dataframe.columns\" y le asignamos una lista con los nombres de las columnas.\n",
        "# en este caso, los nombres de las columnas son:\n",
        "\n",
        "headers = [\"symboling\",\"normalized-losses\",\"make\",\"fuel-type\",\"aspiration\", \"num-of-doors\",\"body-style\",\n",
        "         \"drive-wheels\",\"engine-location\",\"wheel-base\", \"length\",\"width\",\"height\",\"curb-weight\",\"engine-type\",\n",
        "         \"num-of-cylinders\", \"engine-size\",\"fuel-system\",\"bore\",\"stroke\",\"compression-ratio\",\"horsepower\",\n",
        "         \"peak-rpm\",\"city-mpg\",\"highway-mpg\",\"price\"]\n",
        "\n",
        "df.columns = headers\n",
        "print(\"headers\\n\", headers)\n",
        "df.head(10)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Debemos reemplazar los símbolos de \"?\" con NaN (Not a Number) para que el método dropna() pueda eliminar los valores faltantes.\n",
        "#Para esto, utilizamos el método \"dataframe.replace(A, B, inplace = True)\" donde A es el valor que queremos reemplazar y B el valor por el que lo queremos reemplazar, por ejemplo:\n",
        "df1=df.replace('?',np.NaN)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ahora, podemos eliminar las filas que no tienen datos completos utilizando el método \"dataframe.dropna(subset=[\"columna\"], axis=0, inplace = True)\"\n",
        "#En este caso, el argumento subset indica que queremos eliminar las filas que no tienen datos en la columna \"price\", y el argumento axis indica que queremos eliminar las filas (axis=0), no las columnas (axis=1).\n",
        "df=df1.dropna(subset=[\"price\"], axis=0)\n",
        "df.head(20)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "#Ahora, podemos guardar el dataframe en un archivo csv utilizando el método \"dataframe.to_csv(\"nombre_del_archivo.csv\", index=False)\".\n",
        "#El argumento index=False indica que no queremos guardar el índice del dataframe en el archivo csv.\n",
        "df.to_csv(\"automobile.csv\", index=False)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 2: Data Wrangling"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Pre-processing Data in Python\n",
        "El pre-procesamiento de datos es un paso necesario en el análisis de datos. Este se define como el proceso de convertir mapear datos de su forma inicial de adquisición a otro formato, con el propósito de preparar esos datos para analizarlos posteriormente. Comúnmente se le conoce como \"data cleaning\" o \"data wrangling\".\n",
        "\n",
        "- Identificar y manejar valores faltantes\n",
        "- Formateo de datos\n",
        "- Normalización de datos (centering/scaling)\n",
        "- Agrupación de datos\n",
        "- Convertir valores categóricos a variables numéricas"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Dealing with Missing Values in Python\n",
        "La falta de valores es un problema importante en el análisis de datos, estos pueden presentarse con los valores \"?\", \"NaN\", \"N/A\", simplemente una celda vacía, sin valor. Afortunadamente existen diversas formas de lidiar con este problema. El primero, sería revisar si el equipo de adquisición de datos tiene manera de regresar a recabar los datos faltantes. Otra opción es simplemente eliminar la variable completa, o el registro completo donde se presentan los valores faltantes, dependiendo del nivel de importancia de estos valores y qué tanto afectarían al análisis posterior. Sin embargo, una posible mejor solución sería reemplazar los datos faltantes, en vez de eliminar por completo una variable u observación de datos, ya que con esta estrategia no se desperdician datos; entendiendo que la precisión de los datos podría verse afectada.\n",
        "\n",
        "1. Revisar con el equipo de adquisición de datos si puede conseguir los valores faltantes desde la fuente.\n",
        "2. Eliminar los datos faltantes.\n",
        "   1. Eliminar la variable completa que contiene los datos faltantes.\n",
        "   2. Eliminar la observación completa que contiene los datos faltantes.\n",
        "3. Reemplazar los datos faltantes.\n",
        "   1. Reemplazar los datos con la media del resto (o algún grupo particular) de valores.\n",
        "   2. Reemplazar los datos por frecuencia, con la moda del resto (o algún grupo particular) de valores.\n",
        "   3. Reemplazar los datos por medio de otras funciones.\n",
        "4. Mantener los datos faltantes sin modificación."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para eliminar valores faltantes en otras columnas, podemos reemplazarlos por el promedio de la columna utilizando el método ```dataframe.fillna(dataframe.mean(), inplace=True)```\n",
        "\n",
        "También podemos utilizar ```dataframe.dropna()``` para eliminar las observaciones o variables que no tienen datos completos. Para eliminar las observaciones, utiliza \"axis=0\", para eliminar las variables, utiliza \"axis=1\". Para reemplazar valores faltantes, la librería Pandas utiliza el método ```dataframe.replace(A, B, inplace = True)``` donde A es el valor que queremos reemplazar y B el valor por el que lo queremos reemplazar, por ejemplo: ```df.replace(\"?\", np.nan, inplace = True)```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Data Formatting in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Los datos pueden ser recolectados desde diferentes fuentes y almacenados en diferentes formatos. Convertir los datos al mismo estandard de formato es un paso importante para el análisis de datos, pues permite comparar los datos de diferentes fuentes con mayor facilidad. En el curso ponen como ejemplo la forma en que diferentes fuentes se refieren a la ciudad de Nueva York, por ejemplo: \"NY\", \"New York\", \"New York City\", \"NYC\", etc. Para poder comparar los datos de diferentes fuentes, es necesario convertirlos al mismo formato, por ejemplo: \"New York\". Otro ejemplo sería el análisis de datos numéricos en un formato particular, como podría ser la conversión de millas por hora a kilómetros por hora. Esto se puede hacer con una simple operación aritmética: ```df[\"kph\"] = df[\"mph\"] * 1.60934```\n",
        "\n",
        "En este caso, el dataframe \"df\" tiene una columna llamada \"mph\" que contiene los datos en millas por hora, y queremos convertirlos a kilómetros por hora. Para esto, creamos una nueva columna llamada \"kph\" y le asignamos los datos de la columna \"mph\" multiplicados por 1.60934.\n",
        "\n",
        "Para el análisis de datos, es muy importante utilizar el tipo de datos correcto para cada columna. Por ejemplo, en el video muestran una base de datos que contiene la columna \"price\", sin embargo, para realizar operaciones aritméticas sobre esta columna, primero debemos convertirla a un tipo de dato numérico, como float o int. Python utiliza varios tipos de datos, algunos de estos son: int, float, string, boolean, list, tuple, dictionary, etc.Data formatting nos ayuda a convertir los datos de un tipo de dato a otro, por ejemplo, podemos convertir un string a un int o un float, para su correcto análisis. Para identificar el tipo de dato de una columna, puedes utilizar ```dataframe.dtypes()```, y meter como argumento el nombre de la columna que quieres analizar. En caso de tener un tipo de dato incorrecto, el método ```dataframe.astype()``` nos permite convertir el tipo de dato de una columna. Por ejemplo, ```dataframe[\"price\"] = dataframe[\"price\"].astype(\"float\")```"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Data Normalization in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Las bases de datos suelen contar con datos muy variados, y por lo tanto, el rango de valores de cada columna también es muy variado. Para poder comparar los datos de diferentes columnas, es necesario normalizar los datos, es decir, convertirlos a un rango de valores común, esto facilita la comparación de los datos. La normalización de los datos también ayuda en aspectos computacionales, pues permite que los algoritmos de aprendizaje automático funcionen de manera más eficiente. Por otro lado, cuando una variable tiene un rango de valores muy grande, puede influir en mayor medida en el resultado de un modelo de aprendizaje automático, sin ser necesariamente de mayor importancia para predecir una variable objetivo. Existen diferentes métodos para normalizar los datos, como Simple Feature Scaling, Min-Max Scaling o Z-Score Scaling, a continuación se describen:\n",
        "- Para normalizar los datos utilizando Simple Feature Scaling, podemos utilizar el método \".max\" de Pandas para dividir todos los valores de una columna entre el valor máximo de esa misma columna, de la siguiente manera: ```dataframe[\"columna\"] = dataframe[\"columna\"] / dataframe[\"columna\"].max()```\n",
        "- Para normalizar los datos utilizando Min-Max Scaling, podemos utilizar el método \".min\" de Pandas para restar el valor mínimo de una columna a todos los valores de esa misma columna, y luego dividir el resultado entre el valor máximo de esa misma columna, de la siguiente manera: ```dataframe[\"columna\"] = (dataframe[\"columna\"] - dataframe[\"columna\"].min()) / (dataframe[\"columna\"].max() - dataframe[\"columna\"].min())```\n",
        "- Para normalizar los datos utilizando Z-Score scaling, podemos utilizar el método \".mean\" de Pandas para restar el valor promedio de una columna a todos los valores de esa misma columna, y luego dividir el resultado entre la desviación estándar de esa misma columna, de la siguiente manera: ```dataframe[\"columna\"] = (dataframe[\"columna\"] - dataframe[\"columna\"].mean()) / dataframe[\"columna\"].std()```\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Binning in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Se le conoce a *binning* cuando se agrupan valores en ciertas categorías. Por ejemplo, si tenemos una columna con edades, podemos agrupar a las personas en categorías de acuerdo a su edad, por ejemplo: niños, adolescentes, adultos y ancianos. Numpy cuenta con el método linspace, el cual nos permite crear un array con valores equidistantes, esto nos ayuda a generar el inicio y término de cada bin o grupo. El método linspace se implementa de la siguienta manera: ```linspace(start_value, end_value, numbers_generated)```. Por ejemplo, si queremos crear un array con 4 valores equidistantes entre 0 y 100, lo hacemos de la siguiente manera:\n",
        "```bins_4 = np.linspace(min(df[\"price\"]), max(df[\"price\"]), 4)```\n",
        "\n",
        "En Python, podemos realizar *binning* utilizando el método ```pd.cut(dataframe[\"columna\"], bins, labels, include_lowest)```. El argumento bins indica los valores de inicio y término de cada bin, el argumento labels indica el nombre de cada bin, y el argumento include_lowest indica si el primer valor de cada bin debe ser incluido o no; por ejemplo:\n",
        "- ```group_names = ['Low', 'Medium', 'High']```\n",
        "- ```df['price-binned'] = pd.cut(df['price'], bins_4, labels=group_names, include_lowest=True )```\n",
        "\n",
        "Por último, una excelente herramienta para visualizar la agupación de los datos es utilizando histogramas. Para crear un histograma, utilizamos el método ```dataframe[\"columna\"].hist()```.\n",
        "En este caso, queremos visualizar la distribución de los precios de los autos, por lo que utilizamos el método ```df[\"price-binned\"].hist()```.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Turning categorical variables into quantitative variables in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La mayoría de los modelos estadísticos solo aceptan números como entrada, por lo que debemos convertir datos de variables categóricas a valores numéricos. Una técnica común para convertir variables categóricas a valores numéricos es utilizar el método *One Hot Encoding*. En este método, cada valor de la variable categórica se convierte en una nueva columna y se asigna un valor de 0 o 1 a la columna.\n",
        "\n",
        "En la librería Pandas, podemos convertir variables categóricas a valores numéricos utilizando el método ```pd.get_dummies([\"columna\"])```. Por ejemplo, si queremos convertir la variable \"fuel-type\" a valores numéricos, podemos utilizar el método \"get_dummies\" de la siguiente manera:\n",
        "```dummy_variable_1 = pd.get_dummies(df[\"fuel-type\"])```. Después, podemos agregar esta variable al dataframe original utilizando el método ```dataframe.join```, y eliminar la variable \"fuel-type\" del dataframe original utilizando el método \"dataframe.drop\". Ahora nuestro dataframe contiene puras variables numéricas, lo que facilita el análisis de los datos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 3: Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Exploratory Data Analysis"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El EDA (Exploratory Data Analysis), o análisis de datos exploratorio, es un paso importante en el análisis de datos. El objetivo del EDA es resumir las características principales de un conjunto de datos utilizando métodos visuales para entender mejor los datos. El EDA nos ayuda a:\n",
        "1. Entender mejor los datos que estamos utilizando.\n",
        "2. Obtener ideas sobre la relación entre diferentes variables y la respuesta que estamos buscando.\n",
        "3. Desarrollar habilidades, técnicas y comprensión para la posterior modelación.\n",
        "4. Identificar variables importantes para el modelo; identificar variables que no aportan información y pueden ser eliminadas del modelo; o identificar y tratar valores atípicos y valores faltantes.\n",
        "\n",
        ">En el curso de IBM, la pregunta a responder es: ¿cuáles son las características más importantes que determinan el precio de un auto?\n",
        "\n",
        "Existen diferentes técnicas para realizar el EDA, en el curso se mostrarán las siguientes:\n",
        "1. Estadísticas descriptivas.\n",
        "2. Agrupación básica de datoos, utilizando ```groupby```.\n",
        "3. ANOVA (Análisis de varianza).\n",
        "4. Correlación y dependencia.\n",
        "5. Correlación avanzada, particularmente, la correlación de Pearson y la correlación de heatmaps."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Descriptive Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En este video se habla de cómo analizar un dataset por medio de estadística descriptiva. El primer paso mencionado es utilizando el método ```dataframe.describe()``` para obtener estadísticas básicas de las columnas numéricas del dataframe. El método ```dataframe.describe()``` nos devuelve estadísticas básicas de las columnas numéricas del dataframe, como el conteo de observaciones, el promedio, la desviación estándar, el mínimo, el máximo y los cuartiles. Cualquier valor *NaN (Not a Number)* no se muestra en el método ```dataframe.describe()```. Este método de una idea clara, de forma muy rápida, de la distribución de las variables numéricas.\n",
        "\n",
        "Sin embargo, tu dataset podría también contener variables categóricas, como el tipo de combustible, el número de puertas, el tipo de motor, etc. El método ```dataframe.value_counts()``` nos devuelve el conteo de cada valor único en una columna. Por ejemplo, podemos utilizar el método ```dataframe.value_counts()``` para contar el número de observaciones con cada tipo de motor.\n",
        "\n",
        "Una excelente herramienta para visualizar los datos de las variables numéricas es el método ```dataframe.boxplot(column=\"columna\")```. Este método nos devuelve un gráfico de caja para cada columna numérica del dataframe. El gráfico de caja nos muestra la distribución de los datos a través de los cuartiles. El eje horizontal del gráfico de caja representa las diferentes columnas numéricas del dataframe. El eje vertical del gráfico de caja representa el valor de las observaciones. El rango intercuartil (Rango intercuartil = Q3 - Q1) se representa con la línea dentro del gráfico de caja; en donde el cuartil inferior (Q1) se representa con la línea inferior del gráfico de caja; y el cuartil superior (Q3) se representa con la línea superior del gráfico de caja. El valor mínimo se representa con un punto en la parte inferior del gráfico de caja. El valor máximo se representa con un punto en la parte superior del gráfico de caja. Los puntos en el gráfico de caja representan los valores atípicos.\n",
        "\n",
        "Otro gráfico de gran utilidad para visualizar la relación entre dos variables es el gráfico de dispersión, o *scatter plot*. El método ```dataframe.plot(x=\"columna1\", y=\"columna2\", kind=\"scatter\")``` nos devuelve un gráfico de dispersión. La variable independiente, o predictor, se representa en el eje horizontal del gráfico de dispersión. La variable dependiente, o target, se representa en el eje vertical del gráfico de dispersión. En el ejecmplo del video, el predictor es el precio del auto, y el target es el tamaño del motor. Utilizan la función \"scatter\" de la librería ```matplotlib.pyplot``` para crear el gráfico de dispersión."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### GroupBy in Python"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Otra técnica de análisis de datos importante es la agrupación de datos. En Python, podemos agrupar datos utilizando el método ```dataframe.groupby([\"columna\"])```, donde \"columna\" es el nombre de la columna por la que queremos agrupar los datos. El método ```groupby``` se utiliza en variables categóricas, es decir, agrupa los datos en subconjuntos que tienen el mismo valor en la columna por la que se agrupan. Se puede agrupar por más de una columna, por ejemplo: ```df[['columna1', 'columna2']].groupby(['columna1', 'columna2'])```. "
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# Un ejemplo de lo mencionado anteriormente es el siguiente:\n",
        "df_test = df[['drive-wheels','body-style','price']]\n",
        "df_grp = df_test.groupby(['drive-wheels'],as_index=False).mean()\n",
        "df_grp"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Lo anterior nos muestra el promedio de los precios de los autos agrupados por el tipo de tracción (drive-wheels). Para facilitar la visualización de los datos, podemos utilizar el método ```groupby.pivot(\"columna1\", \"columna2\")``` para convertir los datos en una tabla. En este caso, la ```columna1``` es el índice de la tabla y la ```columna2``` es la columna de los datos. El resultado es una tabla con los datos agrupados por la columna1 y la columna2."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "df_pivot = df_grp.pivot(index='drive-wheels',columns='body-style')\n",
        "df_pivot"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "También podemos utilizar el método ```groupby.pivot_table()``` para convertir los datos en una tabla. En este caso, el método ```pivot_table``` tiene los mismos argumentos que el método \"pivot\".El resultado es una tabla con los datos agrupados por la ```columna1``` y la ```columna2```."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "# A continuación, se muestra un ejemplo del método pivot_table():\n",
        "df_pivot = df_grp.pivot_table(index='drive-wheels',columns='body-style')\n",
        "df_pivot\n",
        "\n",
        "# Lo anterior sería similar a una hoja de cálculo, donde tenemos los datos agrupados en filas y columnas, como las que estamos acostumbrados con Excel."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {},
      "outputs": [],
      "source": [
        "Por último, otra gran herramienta de visualización de datos son los mapas de calor, o heatmaps.\n",
        "# En el video, utilizan el método pcolor de la librearía pyplot de matplotlib para crear un mapa de calor, convirtiendo la pivot table a un gráfico visual.\n",
        "\n",
        "plt.pcolor(df_pivot, cmap='RdBu')\n",
        "plt.colorbar()\n",
        "plt.show()\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Analysis of Variance ANOVA"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "ANOVA (Análisis de Varianza), es un método estadístico que nos permite encontrar la correlación entre diferentes grupos de una variable categorica. El F-test score mide cuán diferentes son los promedios de los grupos, mientras que el p-value nos dice la significancia de los resultados. Si el p-value es pequeño (menor a 0.05), y el F-test es elevado (mayor a 1), entonces podemos decir que la correlación de una variable categórica es significativa.\n",
        "\n",
        "En el video del curso, se utiliza el método \"stats.f_oneway\" de la librería scipy para calcular el ANOVA, es decir, para comparar los promedios de los grupos \"make\" y \"price\". El método \"stats.f_oneway\" toma como argumentos los grupos que queremos comparar, y devuelve el F-test score y el p-value. Primero, entre los grupos \"honda\" y \"subaru\", y luego entre los grupos \"honda\" y \"jaguar\".En el caso de \"Honda\" y \"Subaru\", el p-value es muy pequeño, y el F-test score es muy alto, por lo que podemos decir que la correlación entre \"make\" y \"price\" es significativa. En el caso de \"Honda\" y \"Jaguar\", el p-value es muy grande, y el F-test score es muy bajo, por lo que podemos decir que la correlación entre \"make\" y \"price\" no es significativa.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Correlation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La correlación mide hasta qué punto son interdependientes dos o más variables. Por ejemplo, se sabe que fumar tiene una correlación con el cáncer de pulmón, ya que, fumar aumenta las probabilidades de contraer cáncer de pulmón. Otro ejemplo es el uso de paraguas y la lluvia, ya que, a mayor precipitación, más personas usan paraguas. Por lo que se puede decir que existe una correlación entre el uso de paraguas y la lluvia. Sin embargo, es importante entender que la existencua de una correlación no es lo mismo que la existencia de una causalidad. Es decir, el hecho de que exista una correlación entre la lluvia y el uso de paraguas, no significa que la lluvia cause que la gente use paraguas. No existe suficiente información para afirmar que la lluvia causa que la gente use paraguas, o que el uso del paraguas cause que llueva.\n",
        "\n",
        "Una forma de visualizar la correlación entre dos variables es utilizando un scatter plot, incluyendo una línea de regresión. La línea de regresión nos permite ver la tendencia de los datos, y nos ayuda a determinar si existe una correlación entre las variables. En el ejemplo del curso, se utiliza el método ```regplot()``` de la librería seaborn para crear un scatter plot con una línea de regresión, evaluando las variables ```engine-size``` y ```price```. En ese ejemplo, se puede observar que existe una correlación positiva entre las variables, ya que, a mayor tamaño del motor, mayor es el precio del auto.\n",
        "\n",
        "En el curso se muestra un segundo ejemplo, utilizando las variables ```highway-mpg``` y ```price```. En este caso, se puede observar que existe una correlación negativa entre las variables, ya que, a mayor rendimiento en carretera, menor es el precio del auto. Sin embargo, ambos ejemplos muestran que existe una correlación fuerte entre las variables, ya que, a mayor tamaño del motor, el precio del auto aumenta en una proporción similar, y a mayor rendimiento en carretera, el precio del auto disminuye en una proporción similar.\n",
        "\n",
        "Por último, se muestra un tercer ejemplo, utilizando las variables ```peak-rpm``` y ```price```. En este caso, se puede observar que no existe una correlación entre las variables, ya que, a mayor revoluciones por minuto, el precio del auto no aumenta o disminuye en una proporción similar."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Correlation - Statistics"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "En el último video del módulo se presentan algunos métodos de correlación estadística. Una forma de medir la fuerza de correlación entre variables numéricas continuas es utilizando el método Pearson Correlation. Este método entrega dos valores: el coeficiente de correlación y el valor p (*p-value*). El coeficiente de correlación se representa con un valor numérico dentro del rango de -1 a 1, donde:\n",
        "- 1: correlación positiva perfecta\n",
        "- 0: no hay correlación\n",
        "- -1: correlación negativa perfecta\n",
        "\n",
        "El valor p es el valor de probabilidad de que la correlación entre las dos variables sea estadísticamente significativa. El valor p se representa con un número entre 0 y 1, donde:\n",
        "- Un valor p de 0.001 o menor es considerado fuertemente significativo\n",
        "- Un valor p de 0.05 o menor es considerado moderadamente significativo\n",
        "- Un valor p de 0.1 o menor es considerado débilmente significativo\n",
        "- Un valor p de 0.1 o mayor es considerado no significativo\n",
        "\n",
        "Por lo tanto, podemos declarar que dos variables son fuertemente correlacionadas cuando el coeficiente de correlación es cercano a 1 o -1, y el valor p es menor a 0.001.\n",
        "\n",
        "En el video, se utiliza el paquete ```stats``` de la librería ```scipy``` para calcular el coeficiente de correlación de Pearson y el valor p, específicamente, utilizando el método ```stats.pearsonr()```. En el ejemplo, analizan la correlación entre las variables ```horsepower``` y ```price```, donde el coeficiente de correlación es 0.81 y el valor p es 9.35e-48, lo que indica que la correlación entre estas dos variables es fuertemente significativa, ya que el valor p es menor a 0.001, y el coeficiente de correlación es cercano a 1. Por último, para analizar la correlación entre todas las variables de un dataframe, se puede graficar un heatmap para visualizar la correlación directa entre cada una de las variables.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "## Module 4: Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Model Development"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El módulo 4 del curso de Data Analysis with Python de IBM, nos muestra cómo desarrollar un modelo para predecir el precio de un automóvil. A lo largo del módulo se explican los siguientes temas.\n",
        "1. Regresión lineal simple y múltiple\n",
        "2. Evaluación de modelos con herramientas de visualización\n",
        "3. Regresión polinomial y pipelines\n",
        "4. Métricas de evaluación de modelos (R-Squared y MSE)\n",
        "5. Predicción y toma de decisiones\n",
        " \n",
        "Y por último, veremos cómo responder la pregunta:\n",
        "<br><em>¿Cómo determinar un precio justo para un automóvil usado?</em>\n",
        " \n",
        "Un modelo es una ecuación matemática utilizada para predecir un valor, dados otros valores, por medio de relacionar variables independientes con una variable dependiente. Mientras más relevante sean los datos dados, más preciso será el modelo."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Linear and Multiple Linear Regression"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "La regresión lineal es un método para aproximar la relación entre dos variables, una variable dependiente y una variable independiente. El objetivo de la regresión lineal es encontrar la mejor línea que se ajuste a los datos. La línea de regresión lineal simple se define por la siguiente ecuación: ```yhat = a + b  x```, donde yhat es la variable dependiente, a es el punto de intersección con el eje y, b es la pendiente de la línea, y x es la variable independiente. La pendiente y el punto de intersección se calculan utilizando el método de mínimos cuadrados. La regresión lineal múltiple se refiere a la regresión lineal cuando hay más de una variable independiente. En Python, podemos utilizar la librería Scikit-learn para crear modelos de regresión lineal, importando la clase ```LinearRegression``` del módulo ```linear_model```."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "##### Model Evaluation using Visualization"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Regression plots son gráficos que nos permiten visualizar la relación entre dos variables, y la tendencia de la relación (positiva o negativa). El eje horizontal es la variable independiente, y el eje vertical es la variable dependiente. El método regplot() de seaborn nos permite crear un gráfico de dispersión con una línea de regresión.\n",
        "En el siguiente ejemplo, utilizamos el método regplot() para visualizar la relación entre el precio y el tamaño del motor.\n",
        "```sns.regplot(x=\"engine-size\", y=\"price\", data=df)```\n",
        "```plt.ylim(0,)```\n",
        "```plt.show()```\n",
        "\n",
        "Se espera que los resultados tenga una media de cero y una varianza constante.\n",
        "\n",
        "También podemos crear una Residual Plot, que es un gráfico que nos permite visualizar la diferencia entre los valores reales y los valores predichos. El eje horizontal es la variable independiente, y el eje vertical es la diferencia entre los valores reales y los valores predichos. El método residplot() de seaborn nos permite crear un gráfico de dispersión con una línea de regresión. Un gráfico de dispersión es un gráfico que muestra los valores de dos variables para un conjunto de datos. Los histogramas son gráficos que nos permiten visualizar la distribución de una variable discreta o continua.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Polynomial Regression and Pipelines"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Cuando un modelo de regresión lineal no es el más adecuado para predecir los valores de una variable, podemos utilizar un modelo de regresión polinomial. El método de regresión polinomial es muy similar al de regresión lineal, pero en lugar de ajustar una línea recta, ajusta una curva polinomial. Esto es lo que se obtiene al otorgar un grado de mayor a 1 a las variables independientes. Un modelo polinomial puede ser cuadrático (grado 2), cúbico (grado 3), etc. En Python podemos utilizar la función polyfit de la librería numpy para ajustar un modelo polinomial a los datos. Tabmién podemos utilizar el método \"PolynomialFeatures\" de la librería \"preprocessing\" de scikit-learn para crear un modelo de regresión polinomial. El método \"PolynomialFeatures\" transforma los datos en un polinomio de grado n, donde n es el grado del polinomio que queremos crear."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Measures for In-Sample Evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "Para medir el desempeño de un modelo de regresión, utilizamos el método MSE (Mean Squared Error), que es el promedio de los errores al cuadrado, y el método R^2 (R-squared), que es la proporción de la variación de la variable dependiente que es explicada por una variable independiente o variables en un modelo de regresión lineal. El MSE es un buen indicador para saber si el modelo es bueno o no, ya que es un error cuadrático medio, por lo que los errores más grandes se penalizan más que los errores más pequeños. Por lo tanto, el objetivo es minimizar el MSE. El R^2 es un buen indicador para saber si el modelo es bueno o no, ya que es la proporción de la variación de la variable dependiente que es explicada por una variable independiente o variables en un modelo de regresión lineal. Por lo tanto, el objetivo es maximizar el R^2. En Python, podemos calcular el MSE y el R^2 utilizando el módulo metrics de la librería sklearn. El MSE utiliza dos entradas, el valor real y el valor predicho, y el R^2 utiliza dos entradas, el valor real y el valor predicho ajustado. El objetivo es minimizar el MSE y maximizar el R^2."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "#### Prediction and Decision Making"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "El último paso para determinar si nuestro modelo es bueno o no, es utilizar visualización, métricas numéricas para evaluación y comparación de modelos.\t En el video se explican algunas reglas de dedo sobre cómo entender los resultados de las métricas de evaluación de modelos."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---"
      ]
    }
  ],
  "metadata": {
    "colab": {
      "include_colab_link": true,
      "provenance": []
    },
    "kernelspec": {
      "display_name": "Python 3.10.7 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 3
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython3",
      "version": "3.10.7"
    },
    "vscode": {
      "interpreter": {
        "hash": "1cbadf7e30f156f4e0b68f4bca81fa70237e67691d6ebe573bb449e31d409fb8"
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
